#!/bin/bash
#SBATCH --nodes=1                #Número de Nós
#SBATCH --ntasks-per-node=1      #Número de tarefas por Nó
#SBATCH --ntasks=1               #Numero total de tarefas MPI
#SBATCH --cpus-per-task=48       #Numero de threads
#SBATCH -p sequana_gpu_long           #Fila (partition) a ser utilizada
#SBATCH -J DevTrainNN_1            # Nome job
#SBATCH --time=744:00:00         # Obrigatório
#SBATCH --exclusive              # Utilização exclusiva dos nós durante a execução do job

###############################################################
#Exibe os nós alocados para o Job
nodeset -e $SLURM_JOB_NODELIST

workdir=/scratch/ampemi/pedro.santos2/tec_prediction
cd $workdir

. /scratch/ampemi/pedro.santos2/anaconda3/bin/activate
conda activate dscience

module load sequana/current

# Full path to application + application name
application="$(which python)"

# Run options for the application
options="$workdir/main_torch.py --seq_length_min=7200 --step_min=10 --window_train=432 --window_predict=288 --batch_size=50 --epochs=50 --pytorch True --data scin --source=/scratch/ampemi/pedro.santos2/resized_type_1 --cuda True --model unet --work_loader=20"

###############################################################
echo Running on host $(hostname)
echo Path is $PATH
echo Time is $(date)
echo Directory is $(pwd)
echo Slurm job ID is $SLURM_JOB_ID
echo This job runs on the following machines:
echo $SLURM_JOB_NODELIST
echo $options
###############################################################

#configura o numero de threads, de acordo com o parametro definido no Slurm
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

EXEC="$application $options"

srun  -N 1 -c $SLURM_CPUS_PER_TASK $EXEC
