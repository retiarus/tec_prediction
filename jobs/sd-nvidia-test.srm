#!/bin/bash
#SBATCH --nodes=1                #Número de Nós
#SBATCH --ntasks-per-node=1      #Número de tarefas por Nó
#SBATCH --ntasks=1               #Numero total de tarefas MPI
#SBATCH --cpus-per-task=24       #Numero de threads
#SBATCH -p nvidia_dev            #Fila (partition) a ser utilizada
#SBATCH -J DevTrainNN   		     # Nome job
#SBATCH --time=00:20:00          # Obrigatório
#SBATCH --exclusive              # Utilização exclusiva dos nós durante a execução do job

###############################################################
#Exibe os nós alocados para o Job
nodeset -e $SLURM_JOB_NODELIST

workdir=/scratch/ampemi/pedro.santos2/tec_prediction
cd $workdir

. /scratch/ampemi/pedro.santos2/anaconda3/bin/activate
conda activate dscience

# Full path to application + application name
application="$(which python)"

# Run options for the application
options="$workdir/main.py --seq_length_min=4320 --step_min=30 --window_train=96 --window_predict=48 --batch_size=50 --epochs=50 --pytorch True --data scin --model unet --source=/scratch/ampemi/pedro.santos2/resized_type_1 --cuda True"

###############################################################
echo Running on host $(hostname)
echo Path is $PATH
echo Time is $(date)
echo Directory is $(pwd)
echo Slurm job ID is $SLURM_JOB_ID
echo This job runs on the following machines:
echo $SLURM_JOB_NODELIST
###############################################################

#configura o numero de threads, de acordo com o parametro definido no Slurm
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

EXEC="$application $options"

srun  -N 1 -c $SLURM_CPUS_PER_TASK $EXEC
